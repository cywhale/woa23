{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "zarr_group_path = f'{data_dir}/test'\n",
    "chunk_sizes = {'time_periods': 1, 'parameters': 1, 'depth': 8, 'lat': 90, 'lon': 360}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate creating and writing the initial dataset to Zarr\n",
    "ds_initial = xr.Dataset(\n",
    "    {\n",
    "        'mn': (('time_periods', 'parameters', 'depth', 'lat', 'lon'), np.random.rand(1, 1, 57, 720, 1440)),\n",
    "        'an': (('time_periods', 'parameters', 'depth', 'lat', 'lon'), np.random.rand(1, 1, 57, 720, 1440))\n",
    "    },\n",
    "    coords={\n",
    "        'time_periods': ['1'],\n",
    "        'parameters': ['temperature'],\n",
    "        'depth': np.linspace(0, 1500, 57),\n",
    "        'lat': np.linspace(-90, 90, 720),\n",
    "        'lon': np.linspace(-180, 180, 1440)\n",
    "    }\n",
    ")\n",
    "ds_initial = ds_initial.chunk(chunk_sizes)\n",
    "ds_initial.to_zarr(zarr_group_path, mode='w')\n",
    "\n",
    "# Verify the initial dataset\n",
    "dt1 = xr.open_zarr(zarr_group_path, consolidated=False)\n",
    "print(dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run has non-na values\n",
      "Second run has non-na values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7fc9b1cb8940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to reindex and combine new data\n",
    "def reindex_and_combine_existing_with_new(existing_ds, new_ds, param_name, period_key):\n",
    "    if param_name not in existing_ds.parameters.values:\n",
    "        new_params = np.append(existing_ds.parameters.values, param_name)\n",
    "        existing_ds = existing_ds.reindex(parameters=new_params, fill_value=np.nan)\n",
    "\n",
    "    if period_key not in existing_ds.time_periods.values:\n",
    "        new_periods = np.append(existing_ds.time_periods.values, period_key)\n",
    "        existing_ds = existing_ds.reindex(time_periods=new_periods, fill_value=np.nan)\n",
    "\n",
    "    for var in new_ds.data_vars:\n",
    "        existing_ds[var] = existing_ds[var].combine_first(new_ds[var])\n",
    "\n",
    "    return existing_ds\n",
    "\n",
    "# Simulate loading new data and combining it with existing data\n",
    "param_name = 'temperature'\n",
    "period_key = '2'\n",
    "ds_new = xr.Dataset(\n",
    "    {\n",
    "        'mn': (('time_periods', 'parameters', 'depth', 'lat', 'lon'), np.random.rand(1, 1, 57, 720, 1440)),\n",
    "        'an': (('time_periods', 'parameters', 'depth', 'lat', 'lon'), np.random.rand(1, 1, 57, 720, 1440))\n",
    "    },\n",
    "    coords={\n",
    "        'time_periods': [period_key],\n",
    "        'parameters': [param_name],\n",
    "        'depth': np.linspace(0, 1500, 57),\n",
    "        'lat': np.linspace(-90, 90, 720),\n",
    "        'lon': np.linspace(-180, 180, 1440)\n",
    "    }\n",
    ")\n",
    "ds_new = ds_new.chunk(chunk_sizes)\n",
    "\n",
    "# Open the existing Zarr store and combine with new data\n",
    "ds_existing = xr.open_zarr(zarr_group_path, consolidated=True)\n",
    "ds_combined = reindex_and_combine_existing_with_new(ds_existing, ds_new, param_name, period_key)\n",
    "\n",
    "# Verify combined data\n",
    "dt1_sel = ds_combined.sel(parameters='temperature', time_periods='1')\n",
    "t_mn1 = dt1_sel['mn'].values\n",
    "if np.any(np.isfinite(t_mn1)):\n",
    "    print(\"First run has non-na values\")\n",
    "else:\n",
    "    print(\"First run all na values\")\n",
    "\n",
    "dt2_sel = ds_combined.sel(parameters='temperature', time_periods='2')\n",
    "t_mn2 = dt2_sel['mn'].values\n",
    "if np.any(np.isfinite(t_mn2)):\n",
    "    print(\"Second run has non-na values\")\n",
    "else:\n",
    "    print(\"Second run all na values\")\n",
    "\n",
    "# Save the updated combined dataset back to the Zarr store\n",
    "ds_combined.to_zarr(zarr_group_path, mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 946MB\n",
      "Dimensions:       (time_periods: 1, parameters: 1, depth: 57, lat: 720,\n",
      "                   lon: 1440)\n",
      "Coordinates:\n",
      "  * depth         (depth) float64 456B 0.0 26.79 53.57 ... 1.473e+03 1.5e+03\n",
      "  * lat           (lat) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * lon           (lon) float64 12kB -180.0 -179.7 -179.5 ... 179.5 179.7 180.0\n",
      "  * parameters    (parameters) <U11 44B 'temperature'\n",
      "  * time_periods  (time_periods) <U1 4B '1'\n",
      "Data variables:\n",
      "    an            (time_periods, parameters, depth, lat, lon) float64 473MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    mn            (time_periods, parameters, depth, lat, lon) float64 473MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved dataset\n",
    "dt1 = xr.open_zarr(zarr_group_path, consolidated=True)\n",
    "print(dt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All na values\n",
      "\n",
      "has non-na values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True:   \n",
    "   dt1_sel = dt1.sel(parameters='temperature', time_periods='1')\n",
    "   t_mn1 = dt1_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    "    \n",
    "   dt2_sel = dt1.sel(parameters='temperature', time_periods='2')\n",
    "   t_mn2 = dt2_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn2)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region method\n",
    "# Function to reindex and combine new data using the `region` argument\n",
    "def reindex_and_combine_using_region(existing_ds, new_ds, param_name, period_key, zarr_store):\n",
    "    if param_name not in existing_ds.parameters.values:\n",
    "        new_params = np.append(existing_ds.parameters.values, param_name)\n",
    "        existing_ds = existing_ds.reindex(parameters=new_params, fill_value=np.nan)\n",
    "\n",
    "    if period_key not in existing_ds.time_periods.values:\n",
    "        new_periods = np.append(existing_ds.time_periods.values, period_key)\n",
    "        existing_ds = existing_ds.reindex(time_periods=new_periods, fill_value=np.nan)\n",
    "\n",
    "    for var in new_ds.data_vars:\n",
    "        # Define the region to update\n",
    "        param_idx = np.where(existing_ds.parameters.values == param_name)[0][0]\n",
    "        period_idx = np.where(existing_ds.time_periods.values == period_key)[0][0]\n",
    "        region = {\n",
    "            'parameters': slice(param_idx, param_idx + 1),\n",
    "            'time_periods': slice(period_idx, period_idx + 1),\n",
    "            'depth': slice(None),\n",
    "            'lat': slice(None),\n",
    "            'lon': slice(None)\n",
    "        }\n",
    "\n",
    "        # Extract the new data for the specific region\n",
    "        new_data = new_ds[var].data[0, 0, :, :, :]\n",
    "\n",
    "        # Create a temporary DataArray with the correct dimensions\n",
    "        temp_da = xr.DataArray(\n",
    "            new_data,\n",
    "            dims=['depth', 'lat', 'lon'],\n",
    "            coords={\n",
    "                'depth': existing_ds.depth,\n",
    "                'lat': existing_ds.lat,\n",
    "                'lon': existing_ds.lon\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update the existing dataset with the new data in the specified region\n",
    "        print(existing_ds)\n",
    "        print(existing_ds[var])\n",
    "        existing_ds[var].loc[dict(parameters=param_name, time_periods=period_key)] = temp_da\n",
    "\n",
    "        # Write the updated data to Zarr store without re-writing the coordinates\n",
    "        existing_ds[var].to_zarr(zarr_store, region=region)\n",
    "\n",
    "    return existing_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate loading new data and combining it with existing data\n",
    "param_name = 'temperature'\n",
    "period_key = '2'\n",
    "ds_new = xr.Dataset(\n",
    "    {\n",
    "        'mn': (('time_periods', 'parameters', 'depth', 'lat', 'lon'), np.random.rand(1, 1, 57, 720, 1440)),\n",
    "        'an': (('time_periods', 'parameters', 'depth', 'lat', 'lon'), np.random.rand(1, 1, 57, 720, 1440))\n",
    "    },\n",
    "    coords={\n",
    "        'time_periods': [period_key],\n",
    "        'parameters': [param_name],\n",
    "        'depth': np.linspace(0, 1500, 57),\n",
    "        'lat': np.linspace(-90, 90, 720),\n",
    "        'lon': np.linspace(-180, 180, 1440)\n",
    "    }\n",
    ")\n",
    "ds_new = ds_new.chunk(chunk_sizes)\n",
    "\n",
    "# Open the existing Zarr store and combine with new data\n",
    "ds_existing = xr.open_zarr(zarr_group_path, consolidated=True)\n",
    "ds_combined = reindex_and_combine_using_region(ds_existing, ds_new, param_name, period_key, zarr_group_path)\n",
    "\n",
    "# Verify combined data\n",
    "dt1_sel = ds_combined.sel(parameters='temperature', time_periods='1')\n",
    "t_mn1 = dt1_sel['mn'].values\n",
    "if np.any(np.isfinite(t_mn1)):\n",
    "    print(\"First run has non-na values\")\n",
    "else:\n",
    "    print(\"First run all na values\")\n",
    "\n",
    "dt2_sel = ds_combined.sel(parameters='temperature', time_periods='2')\n",
    "t_mn2 = dt2_sel['mn'].values\n",
    "if np.any(np.isfinite(t_mn2)):\n",
    "    print(\"Second run has non-na values\")\n",
    "else:\n",
    "    print(\"Second run all na values\")\n",
    "\n",
    "# Save the updated combined dataset back to the Zarr store\n",
    "ds_combined.to_zarr(zarr_group_path, mode='w')\n",
    "\n",
    "# Verify the saved dataset\n",
    "dt1 = xr.open_zarr(zarr_group_path, consolidated=True)\n",
    "print(dt1)\n",
    "\n",
    "# Test the data after saving\n",
    "if True:   \n",
    "   dt1_sel = dt1.sel(parameters='temperature', time_periods='1')\n",
    "   t_mn1 = dt1_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    "    \n",
    "   dt2_sel = dt1.sel(parameters='temperature', time_periods='2')\n",
    "   t_mn2 = dt2_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn2)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "zarr_group_path = f'{data_dir}/test'\n",
    "# Define paths and parameters\n",
    "grid_resolutions = {'01': '1.00', '04': '0.25'}\n",
    "grid_dir = {'01': '1_degree', '04': '0.25_degree'}\n",
    "parameters = {\n",
    "    't': 'temperature',\n",
    "    's': 'salinity',\n",
    "    'o': 'DOXY',\n",
    "    'O': 'O2S',\n",
    "    'A': 'AOU',\n",
    "    'i': 'silicate',\n",
    "    'p': 'phosphate',\n",
    "    'n': 'nitrate'\n",
    "}\n",
    "time_periods = {\n",
    "    '0': 'annual',\n",
    "    '1': 'january',\n",
    "    '2': 'february',\n",
    "    '3': 'march',\n",
    "    '4': 'april',\n",
    "    '5': 'may',\n",
    "    '6': 'june',\n",
    "    '7': 'july',\n",
    "    '8': 'august',\n",
    "    '9': 'september',\n",
    "    '10': 'october',\n",
    "    '11': 'november',\n",
    "    '12': 'december',\n",
    "    '13': 'winter',\n",
    "    '14': 'spring',\n",
    "    '15': 'summer',\n",
    "    '16': 'autumn'\n",
    "}\n",
    "data_variables = ['an', 'mn', 'dd', 'ma', 'sd', 'se', 'oa', 'gp', 'sdo', 'sea']\n",
    "chunk_sizes = {'time_periods': 1, 'parameters': 1, 'depth': 8, 'lat': 90, 'lon': 360}\n",
    "\n",
    "# Function to initialize Zarr store with required dimensions incrementally\n",
    "def initialize_zarr_store(zarr_group_path, ds, chunk_sizes):\n",
    "    # Initialize the store with only the coordinates\n",
    "    empty_ds = xr.Dataset(coords=ds.coords)\n",
    "    empty_ds.to_zarr(zarr_group_path, mode='w')\n",
    "\n",
    "    # Incrementally add each variable to avoid memory exhaustion\n",
    "    for var in data_variables:\n",
    "        data_shape = (\n",
    "            len(ds.coords['time_periods']),\n",
    "            len(ds.coords['parameters']),\n",
    "            len(ds.coords['depth']),\n",
    "            len(ds.coords['lat']),\n",
    "            len(ds.coords['lon'])\n",
    "        )\n",
    "        temp_data = np.empty(data_shape, dtype=np.float32)\n",
    "        temp_data.fill(np.nan)  # Fill with NaNs to indicate empty data\n",
    "        temp_ds = xr.Dataset({var: (('time_periods', 'parameters', 'depth', 'lat', 'lon'), temp_data)}, coords=ds.coords)\n",
    "        temp_ds = temp_ds.chunk(chunk_sizes)\n",
    "        temp_ds.to_zarr(zarr_group_path, mode='a')\n",
    "        del temp_data, temp_ds  # Clear variables to free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 18kB\n",
      "Dimensions:       (lon: 1440, lat: 720, depth: 57, parameters: 8,\n",
      "                   time_periods: 17)\n",
      "Coordinates:\n",
      "  * lon           (lon) float64 12kB -180.0 -179.7 -179.5 ... 179.5 179.7 180.0\n",
      "  * lat           (lat) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * depth         (depth) float64 456B 0.0 26.79 53.57 ... 1.473e+03 1.5e+03\n",
      "  * parameters    (parameters) <U11 352B 'temperature' 'salinity' ... 'nitrate'\n",
      "  * time_periods  (time_periods) <U2 136B '0' '1' '2' '3' ... '14' '15' '16'\n",
      "Data variables:\n",
      "    *empty*\n"
     ]
    }
   ],
   "source": [
    "ds_initial = xr.Dataset(\n",
    "    coords={\n",
    "        'lon': np.linspace(-180, 180, 1440),\n",
    "        'lat': np.linspace(-90, 90, 720),\n",
    "        'depth': np.linspace(0, 1500, 57),\n",
    "        'parameters': list(parameters.values()),\n",
    "        'time_periods': list(time_periods.keys())  # Use numeric keys for time_periods\n",
    "    }\n",
    ")\n",
    "\n",
    "print(ds_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_zarr_store(zarr_group_path, ds_initial, chunk_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 80GB\n",
      "Dimensions:       (time_periods: 17, parameters: 2, depth: 57, lat: 720,\n",
      "                   lon: 1440)\n",
      "Coordinates:\n",
      "  * depth         (depth) float64 456B 0.0 26.79 53.57 ... 1.473e+03 1.5e+03\n",
      "  * lat           (lat) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * lon           (lon) float64 12kB -180.0 -179.7 -179.5 ... 179.5 179.7 180.0\n",
      "  * parameters    (parameters) <U11 88B 'temperature' 'salinity'\n",
      "  * time_periods  (time_periods) <U2 136B '0' '1' '2' '3' ... '14' '15' '16'\n",
      "Data variables:\n",
      "    an            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    dd            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    gp            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    ma            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    mn            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    oa            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sd            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sdo           (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    se            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sea           (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Verify the initialized Zarr store\n",
    "dt1 = xr.open_zarr(zarr_group_path, consolidated=False)\n",
    "print(dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append a new dataset to the existing Zarr store\n",
    "def append_to_zarr_store(zarr_group_path, nc_file, param_key, period_key):\n",
    "    # Open the existing Zarr store\n",
    "    ds_existing = xr.open_zarr(zarr_group_path, consolidated=False)\n",
    "    \n",
    "    # Load the new dataset from NetCDF with decode_times=False\n",
    "    ds_new = xr.open_dataset(nc_file, decode_times=False)\n",
    "\n",
    "    # Drop unused variables if they exist\n",
    "    ds_new = ds_new.drop_vars(['crs', 'lat_bnds', 'lon_bnds', 'depth_bnds', 'climatology_bounds'], errors='ignore')\n",
    "    \n",
    "    # Rename variables according to data_variables\n",
    "    rename_vars = {f'{param_key}_{var}': var for var in data_variables}\n",
    "    ds_new = ds_new.rename(rename_vars)\n",
    "    \n",
    "    # Remove the time dimension if it exists\n",
    "    for var in data_variables:\n",
    "        if 'time' in ds_new[var].dims:\n",
    "            ds_new[var] = ds_new[var].squeeze('time', drop=True)\n",
    "    ds_new = ds_new.drop_vars('time', errors='ignore')\n",
    "\n",
    "    # Extract the parameter name\n",
    "    param_name = parameters[param_key]\n",
    "\n",
    "    # Ensure the new dataset has the correct coordinates and dimensions\n",
    "    ds_new = ds_new.expand_dims({'parameters': [param_name], 'time_periods': [period_key]})\n",
    "\n",
    "    for var in ds_new.data_vars:\n",
    "        # Define the region to update\n",
    "        # param_idx = list(ds_existing.parameters.values).index(param_name)\n",
    "        # period_idx = list(ds_existing.time_periods.values).index(period_key)   \n",
    "        #region = {\n",
    "        #    'parameters': slice(param_idx, param_idx + 1),\n",
    "        #    'time_periods': slice(period_idx, period_idx + 1),\n",
    "        #    'depth': slice(None),\n",
    "        #    'lat': slice(None),\n",
    "        #    'lon': slice(None)\n",
    "        #}\n",
    "\n",
    "        # Extract the relevant data for the region update\n",
    "        new_data = ds_new[var].data.squeeze()\n",
    "        \n",
    "        # Create a temporary DataArray with the correct dimensions\n",
    "        temp_da = xr.DataArray(\n",
    "            new_data,\n",
    "            dims=['depth', 'lat', 'lon'],\n",
    "            coords={\n",
    "                'depth': ds_existing.depth,\n",
    "                'lat': ds_existing.lat,\n",
    "                'lon': ds_existing.lon\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Update the existing dataset with the new data in the specified region\n",
    "        ds_existing[var].loc[dict(parameters=param_name, time_periods=period_key)] = temp_da\n",
    "\n",
    "    # Write the updated data to the Zarr store without re-writing the coordinates\n",
    "    ds_existing.chunk(chunk_sizes).to_zarr(zarr_group_path, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = '../tmp_data/woa23_decav_t01_04.nc'\n",
    "#ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "#print(ds)\n",
    "append_to_zarr_store(zarr_group_path, nc_file, 't', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 80GB\n",
      "Dimensions:       (time_periods: 17, parameters: 2, depth: 57, lat: 720,\n",
      "                   lon: 1440)\n",
      "Coordinates:\n",
      "  * depth         (depth) float64 456B 0.0 26.79 53.57 ... 1.473e+03 1.5e+03\n",
      "  * lat           (lat) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * lon           (lon) float64 12kB -180.0 -179.7 -179.5 ... 179.5 179.7 180.0\n",
      "  * parameters    (parameters) <U11 88B 'temperature' 'salinity'\n",
      "  * time_periods  (time_periods) <U2 136B '0' '1' '2' '3' ... '14' '15' '16'\n",
      "Data variables:\n",
      "    an            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    dd            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    gp            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    ma            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    mn            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    oa            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sd            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sdo           (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    se            (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sea           (time_periods, parameters, depth, lat, lon) float32 8GB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Verify the initialized Zarr store\n",
    "dt1 = xr.open_zarr(zarr_group_path, consolidated=False)\n",
    "print(dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has non-na values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True:   \n",
    "   dt1_sel = dt1.sel(parameters='temperature', time_periods='1')\n",
    "   t_mn1 = dt1_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = '../tmp_data/woa23_decav_t02_04.nc'\n",
    "append_to_zarr_store(zarr_group_path, nc_file, 't', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has non-na values\n",
      "\n",
      "has non-na values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True:   \n",
    "   dt1_sel = dt1.sel(parameters='temperature', time_periods='1')\n",
    "   t_mn1 = dt1_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    "    \n",
    "   dt2_sel = dt1.sel(parameters='temperature', time_periods='2')\n",
    "   t_mn2 = dt2_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn2)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = '../tmp_data/woa23_decav_s01_04.nc'\n",
    "append_to_zarr_store(zarr_group_path, nc_file, 's', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = '../tmp_data/woa23_decav_s02_04.nc'\n",
    "append_to_zarr_store(zarr_group_path, nc_file, 's', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has non-na values\n",
      "\n",
      "has non-na values\n",
      "\n",
      "has non-na values\n",
      "\n",
      "has non-na values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True:   \n",
    "   dt1_sel = dt1.sel(parameters='temperature', time_periods='1')\n",
    "   t_mn1 = dt1_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    "    \n",
    "   dt2_sel = dt1.sel(parameters='temperature', time_periods='2')\n",
    "   t_mn2 = dt2_sel['mn'].values\n",
    "   if np.any(np.isfinite(t_mn2)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    "\n",
    "   dt3_sel = dt1.sel(parameters='salinity', time_periods='1')\n",
    "   s_mn1 = dt3_sel['mn'].values\n",
    "   if np.any(np.isfinite(s_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")\n",
    "\n",
    "   dt4_sel = dt1.sel(parameters='salinity', time_periods='1')\n",
    "   s_mn1 = dt3_sel['mn'].values\n",
    "   if np.any(np.isfinite(s_mn1)):\n",
    "      print(\"has non-na values\\n\")\n",
    "   else:\n",
    "      print(\"All na values\\n\")                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
