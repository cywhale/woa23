{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkb\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01'] ['TS', 'Oxy', 'Nutrients'] ['annual', 'seasonal', 'monthly']\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "DBUSER = os.getenv('DBUSER')\n",
    "DBPASS = os.getenv('DBPASS')\n",
    "DBHOST = os.getenv('DBHOST')\n",
    "DBPORT = os.getenv('DBPORT')\n",
    "DBNAME = os.getenv('DBNAME')\n",
    "GRIDSET = os.getenv('GRIDSET')\n",
    "TIMESET = os.getenv('TIMESET')\n",
    "PARAMSET = os.getenv('PARAMSET')\n",
    "# print(PARAMSET)\n",
    "pars = [c.strip() for c in PARAMSET.split(',')] # if c.strip() in available_params]))\n",
    "time_periods = [p.strip() for p in str(TIMESET).split(',')]\n",
    "grids = [g.strip() for g in str(GRIDSET).split(',')]\n",
    "db_settings = {\n",
    "    'dbname': DBNAME,\n",
    "    'user': DBUSER,\n",
    "    'password': DBPASS,\n",
    "    'host': DBHOST,\n",
    "    'port': DBPORT\n",
    "}\n",
    "\n",
    "# Function to establish connection to PostgreSQL\n",
    "def connect_db(settings):\n",
    "    conn = psycopg2.connect(**settings)\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    return conn\n",
    "\n",
    "print(grids, pars, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create TS table in PostGIS\n",
    "# When try grd025_annual_ts use temperature FLOAT8, salinity FLOAT8,\n",
    "def create_ts_table(conn, table_name, parameter_set):\n",
    "    with conn.cursor() as cur:\n",
    "        columns = \", \".join([f\"{param} FLOAT\" for param in parameter_set])\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            lon FLOAT,\n",
    "            lat FLOAT,\n",
    "            depth INTEGER,\n",
    "            time_period INTEGER,\n",
    "            {columns},\n",
    "            geom GEOMETRY\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(create_table_query)\n",
    "        \n",
    "        index_geom = f\"CREATE INDEX idx_{table_name}_geom ON {table_name} USING GIST (geom);\"\n",
    "        index_depth = f\"CREATE INDEX idx_{table_name}_depth ON {table_name} (depth);\"\n",
    "        index_time_period = f\"CREATE INDEX idx_{table_name}_time_period ON {table_name} (time_period);\"\n",
    "\n",
    "        cur.execute(index_geom)\n",
    "        cur.execute(index_depth)\n",
    "        cur.execute(index_time_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test read zarr dataset\n",
    "zarr_store_path = '../data/025_degree/annual/TS'\n",
    "res = '04'  # '04' for 0.25-degree, '01' for 1-degree\n",
    "ds = xr.open_zarr(zarr_store_path, consolidated=False)\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate geom for PostGIS using the grid structure\n",
    "def generate_grid_polygon(lon, lat, res):\n",
    "    step = 0.25 if res == '04' else 1.0\n",
    "    lon_min = lon - step / 2\n",
    "    lon_max = lon + step / 2\n",
    "    lat_min = lat - step / 2\n",
    "    lat_max = lat + step / 2\n",
    "    return Polygon([(lon_min, lat_min), (lon_min, lat_max), (lon_max, lat_max), (lon_max, lat_min), (lon_min, lat_min)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_postgis(conn, table_name, data, parameter_set, res, batch_size=100000):\n",
    "    cur = conn.cursor()\n",
    "    count = 0\n",
    "    #with conn.cursor() as cur:\n",
    "    if True:\n",
    "        for time_idx in range(data.dims['time_periods']):\n",
    "            for depth_idx in range(data.dims['depth']):\n",
    "                for lat_idx in range(data.dims['lat']):\n",
    "                    for lon_idx in range(data.dims['lon']):\n",
    "                        lon = float(data['lon'].data[lon_idx])\n",
    "                        lat = float(data['lat'].data[lat_idx])\n",
    "                        depth = int(data['depth'].data[depth_idx])\n",
    "                        time_period = int(data['time_periods'].data[time_idx])\n",
    "\n",
    "                        # Prepare values for all parameters in the parameter_set\n",
    "                        values = []\n",
    "                        for param in parameter_set:\n",
    "                            values.append(float(data['mn'].sel(parameters=param).data[time_idx, depth_idx, lat_idx, lon_idx]))\n",
    "\n",
    "                        # Check if all values are NaN; if so, skip the row\n",
    "                        if all(np.isnan(v) for v in values):\n",
    "                            continue\n",
    "\n",
    "                        geom = generate_grid_polygon(lon, lat, res)\n",
    "                        geom_wkb = wkb.dumps(geom, hex=True)\n",
    "\n",
    "                        # Dynamically create the query string\n",
    "                        query = f\"\"\"\n",
    "                        INSERT INTO {table_name} (lon, lat, depth, time_period, {', '.join(parameter_set)}, geom)\n",
    "                        VALUES (%s, %s, %s, %s, {', '.join(['%s'] * len(parameter_set))}, ST_SetSRID(%s::geometry, 4326));\n",
    "                        \"\"\"\n",
    "\n",
    "                        cur.execute(query, (lon, lat, depth, time_period, *values, geom_wkb))\n",
    "                        count += 1\n",
    "\n",
    "                        if count % batch_size == 0:\n",
    "                            conn.commit()  # Commit every batch_size records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_db = {'01': 'grd1', '04': 'grd025'}  # Two gridded resolutions data: 1-degree and 0.25-degree in WOA23\n",
    "grid_dir = {'01': '1_degree', '04': '025_degree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test res, pars 04 ['04', '01'] ['TS', 'Oxy', 'Nutrients']\n",
      "Test table name and parameter_set grd025_annual_TS ['temperature', 'salinity']\n"
     ]
    }
   ],
   "source": [
    "if True: #__name__ == '__main__' #only Test\n",
    "    res = grids[0]  # '04' for 0.25-degree, '01' for 1-degree\n",
    "    print(\"Test res, pars\", res, grids, pars)\n",
    "\n",
    "    zarr_store_path = f\"../data/{grid_dir[res]}/{time_periods[0]}/{pars[0]}\"\n",
    "    table_name = f\"{grid_db[res]}_{time_periods[0]}_{pars[0]}\"\n",
    "    if res == '04' or pars == 'TS':\n",
    "        parameter_set = ['temperature', 'salinity']  # For TS subgroup\n",
    "    elif pars == 'Oxy': \n",
    "        parameter_set = ['oxygen', 'o2sat', 'AOU']\n",
    "    else:\n",
    "        parameter_set = ['silicate', 'phosphate', 'nitrate']        \n",
    "\n",
    "print(\"Test table name and parameter_set\", table_name, parameter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect_db(db_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 8GB\n",
      "Dimensions:       (time_periods: 1, parameters: 2, depth: 102, lat: 720,\n",
      "                   lon: 1440)\n",
      "Coordinates:\n",
      "  * depth         (depth) float32 408B 0.0 5.0 10.0 ... 5.3e+03 5.4e+03 5.5e+03\n",
      "  * lat           (lat) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n",
      "  * parameters    (parameters) <U11 88B 'temperature' 'salinity'\n",
      "  * time_periods  (time_periods) <U1 4B '0'\n",
      "Data variables:\n",
      "    an            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    dd            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    gp            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    mn            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    oa            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sd            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sdo           (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    se            (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n",
      "    sea           (time_periods, parameters, depth, lat, lon) float32 846MB dask.array<chunksize=(1, 1, 8, 90, 360), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_zarr(zarr_store_path, consolidated=False)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33171/2313205739.py:3: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for time_idx in range(data.dims['time_periods']):\n",
      "/tmp/ipykernel_33171/2313205739.py:4: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for depth_idx in range(data.dims['depth']):\n",
      "/tmp/ipykernel_33171/2313205739.py:5: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for lat_idx in range(data.dims['lat']):\n",
      "/tmp/ipykernel_33171/2313205739.py:6: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for lon_idx in range(data.dims['lon']):\n"
     ]
    }
   ],
   "source": [
    "create_ts_table(conn, table_name)\n",
    "# print(conn)\n",
    "insert_into_postgis(conn, table_name, data, parameter_set, res)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currnet handling:  01 annual TS\n",
      "Create table name:  grd1_annual_TS\n",
      "Read dataset:  ../data/1_degree/annual/TS\n",
      "Start inserting data at date:  2024-09-10 09:42:45.202705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282097/2636093059.py:6: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for time_idx in range(data.dims['time_periods']):\n",
      "/tmp/ipykernel_282097/2636093059.py:7: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for depth_idx in range(data.dims['depth']):\n",
      "/tmp/ipykernel_282097/2636093059.py:8: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for lat_idx in range(data.dims['lat']):\n",
      "/tmp/ipykernel_282097/2636093059.py:9: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for lon_idx in range(data.dims['lon']):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End this insertion:  2024-09-10 14:53:15.184684\n",
      "Currnet handling:  01 annual Oxy\n",
      "Create table name:  grd1_annual_Oxy\n",
      "Read dataset:  ../data/1_degree/annual/Oxy\n",
      "Start inserting data at date:  2024-09-10 14:53:15.429792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282097/2636093059.py:6: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for time_idx in range(data.dims['time_periods']):\n",
      "/tmp/ipykernel_282097/2636093059.py:7: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for depth_idx in range(data.dims['depth']):\n",
      "/tmp/ipykernel_282097/2636093059.py:8: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for lat_idx in range(data.dims['lat']):\n",
      "/tmp/ipykernel_282097/2636093059.py:9: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for lon_idx in range(data.dims['lon']):\n"
     ]
    }
   ],
   "source": [
    "# Main loop to iterate through grids, time_periods, and pars\n",
    "# conn = connect_db(db_settings)\n",
    "# grid_db = {'01': 'grd1', '04': 'grd025'}\n",
    "# grid_dir = {'01': '1_degree', '04': '025_degree'}\n",
    "\n",
    "for res in grids:\n",
    "    for time_period in time_periods:\n",
    "        for par in pars:\n",
    "            # Determine the Zarr store path and table name\n",
    "            zarr_store_path = f\"../data/{grid_dir[res]}/{time_period}/{par}\"\n",
    "            table_name = f\"{grid_db[res]}_{time_period}_{par}\"\n",
    "\n",
    "            # Determine the parameter set based on the subgroup\n",
    "            if res == '04' or par == 'TS':\n",
    "                parameter_set = ['temperature', 'salinity']\n",
    "            elif par == 'Oxy':\n",
    "                parameter_set = ['oxygen', 'o2sat', 'AOU']\n",
    "            else:\n",
    "                parameter_set = ['silicate', 'phosphate', 'nitrate']\n",
    "\n",
    "            # Create the table and index it\n",
    "            print(\"Currnet handling: \", res, time_period, par)\n",
    "            print(\"Create table name: \", table_name)\n",
    "            create_ts_table(conn, table_name, parameter_set)\n",
    "            print(\"Read dataset: \", zarr_store_path)\n",
    "            # Load data from the Zarr store and insert it into the table\n",
    "            data = xr.open_zarr(zarr_store_path, consolidated=False)\n",
    "            print(\"Start inserting data at date: \", datetime.now())\n",
    "            insert_into_postgis(conn, table_name, data, parameter_set, res)\n",
    "            print(\"End this insertion: \", datetime.now())\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the last record in the table\n",
    "def find_last_record(conn, table_name):\n",
    "    query = f\"\"\"\n",
    "    SELECT lon, lat, depth, time_period \n",
    "    FROM {table_name} \n",
    "    ORDER BY depth DESC, time_period DESC, lat DESC, lon DESC \n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "    if result:\n",
    "        print(\"Find last record: \", result)\n",
    "        return {\n",
    "            'lon': result[0],\n",
    "            'lat': result[1],\n",
    "            'depth': result[2],\n",
    "            'time_period': result[3]\n",
    "        }\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from the last record\n",
    "RecoveryFromLastRecord = True\n",
    "if RecoveryFromLastRecord:\n",
    "    for depth_idx, depth in enumerate(data[\"depth\"].data):\n",
    "        for time_idx, time_period_value in enumerate(data[\"time_periods\"].data):\n",
    "            for lat_idx, lat in enumerate(data[\"lat\"].data):\n",
    "                for lon_idx, lon in enumerate(data[\"lon\"].data):\n",
    "                    # Check if this record should be skipped\n",
    "                    if last_record:\n",
    "                        if (\n",
    "                            depth < last_record[\"depth\"]\n",
    "                            or (\n",
    "                                depth == last_record[\"depth\"]\n",
    "                                and time_period_value < last_record[\"time_period\"]\n",
    "                            )\n",
    "                            or (\n",
    "                                depth == last_record[\"depth\"]\n",
    "                                and time_period_value == last_record[\"time_period\"]\n",
    "                                and lat < last_record[\"lat\"]\n",
    "                            )\n",
    "                            or (\n",
    "                                depth == last_record[\"depth\"]\n",
    "                                and time_period_value == last_record[\"time_period\"]\n",
    "                                and lat == last_record[\"lat\"]\n",
    "                                and lon <= last_record[\"lon\"]\n",
    "                            )\n",
    "                        ):\n",
    "                            continue\n",
    "                        last_record = None  # Start processing from here once we pass the last record\n",
    "\n",
    "                    # Insert the data into the database\n",
    "                    insert_into_postgis(conn, table_name, data, parameter_set, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find last record:  (127.625, -47.875, 70.0, 0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'numpy.str_' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lon_idx, lon \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Check if this record should be skipped\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_record:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (depth \u001b[38;5;241m<\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m             (depth \u001b[38;5;241m==\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mtime_period_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_record\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_period\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             (depth \u001b[38;5;241m==\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m time_period_value \u001b[38;5;241m==\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_period\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m lat \u001b[38;5;241m<\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     30\u001b[0m             (depth \u001b[38;5;241m==\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m time_period_value \u001b[38;5;241m==\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_period\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m lat \u001b[38;5;241m==\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m lon \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m last_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         last_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Start processing from here once we pass the last record\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'numpy.str_' and 'int'"
     ]
    }
   ],
   "source": [
    "# Update your main loop to continue from the last record\n",
    "# conn = connect_db(db_settings)\n",
    "# grid_db = {'01': 'grd1', '04': 'grd025'}\n",
    "# grid_dir = {'01': '1_degree', '04': '025_degree'}\n",
    "\n",
    "for res in grids:\n",
    "    for time_period in time_periods:\n",
    "        for par in pars:\n",
    "            table_name = f\"{grid_db[res]}_{time_period}_{par}\"\n",
    "            last_record = find_last_record(conn, table_name)\n",
    "            \n",
    "            # Determine the parameter set\n",
    "            if res == '04' or par == 'TS':\n",
    "                parameter_set = ['temperature', 'salinity']\n",
    "            elif par == 'Oxy':\n",
    "                parameter_set = ['oxygen', 'o2sat', 'AOU']\n",
    "            else:\n",
    "                parameter_set = ['silicate', 'phosphate', 'nitrate']\n",
    "\n",
    "            data = xr.open_zarr(zarr_store_path, consolidated=False)\n",
    "            for depth_idx, depth in enumerate(data['depth'].data):\n",
    "                for time_idx, time_period_value in enumerate(data['time_periods'].data):\n",
    "                    for lat_idx, lat in enumerate(data['lat'].data):\n",
    "                        for lon_idx, lon in enumerate(data['lon'].data):\n",
    "                            # Check if this record should be skipped\n",
    "                            if last_record:\n",
    "                                if (depth < last_record['depth'] or\n",
    "                                    (depth == last_record['depth'] and time_period_value < last_record['time_period']) or\n",
    "                                    (depth == last_record['depth'] and time_period_value == last_record['time_period'] and lat < last_record['lat']) or\n",
    "                                    (depth == last_record['depth'] and time_period_value == last_record['time_period'] and lat == last_record['lat'] and lon <= last_record['lon'])):\n",
    "                                    continue\n",
    "                                last_record = None  # Start processing from here once we pass the last record\n",
    "                            \n",
    "                            # Insert the data into the database\n",
    "                            insert_into_postgis(conn, table_name, data, parameter_set, res)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
